{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Data manipulation libraries\n",
    "import numpy as np\n",
    "np.random.seed(1881)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Data visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# calculating the geographical distance\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Modelling\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Settings for better notebook visualisation\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False) \n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1. Problem Statement\n",
    "\n",
    "Credit card fraud is a serious issue that causes significant financial losses for both individuals and financial institutions. This type of fraud leads to billions of dollars in damages each year, and preventing it is critical for both customer security and the protection of banks' financial structures. Since credit card transactions are performed frequently and at high volumes by users, detecting fraudulent transactions through traditional methods has become quite challenging. At this point, machine learning algorithms and big data analytics provide an effective solution for the early detection and prevention of fraudulent activities.\n",
    "\n",
    "Our dataset consists of credit card transactions made by users. In this project, I will examine the relevant dataset and develop a machine learning model to detect fraudulent transactions.\n",
    "\n",
    "## 1.2. Project Objective\n",
    "\n",
    "The goal of this project is to develop the best-performing model for detecting credit card fraud using machine learning techniques. Specifically:\n",
    "\n",
    "* **Fraud detection:** To build a model that classifies whether the transactions in the dataset are legitimate or fraudulent.\n",
    "* **Comparison of different models:** Compare the performance of different machine learning models such as **Catboost, XGBoost and LightGBM.**\n",
    "* **Use of evaluation metrics:** Instead of accuracy, evaluate the models using more meaningful metrics like precision, recall, F1-score, and ROC AUC, considering class imbalance.\n",
    "\n",
    "# 2. Data Exploration\n",
    "\n",
    "## 2.1. Columns and Their Descriptions\n",
    "\n",
    "* **`trans_date_trans_time`:** Timestamp of the transaction.\n",
    "* **`cc_num`:** Credit card number (encrypted).\n",
    "* **`merchant`:** The store or vendor where the transaction occurred.\n",
    "* **`category`:** Transaction category (e.g., groceries, entertainment).\n",
    "* **`amt`:** Transaction amount.\n",
    "* **`first`:** Cardholder's first name. \n",
    "* **`last`:** Cardholder's last name.\n",
    "* **`gender`:** Cardholder's gender.\n",
    "* **`street`:** Street where the cardholder resides.\n",
    "* **`city`:** City where the cardholder resides.\n",
    "* **`state`:** State where the cardholder resides.\n",
    "* **`zip`:** Postal code of the cardholder.\n",
    "* **`lat`:** Latitude of the cardholder's address.\n",
    "* **`long`:** Longitude of the cardholder's address.\n",
    "* **`city_pop`:** Population of the city where the transaction occurred.\n",
    "* **`job`:** Cardholder's occupation.\n",
    "* **`dob`:** Cardholder's date of birth.\n",
    "* **`trans_num`:** Unique transaction number.\n",
    "* **`unix_time`:** Unix timestamp of the transaction.\n",
    "* **`merch_lat`:** Latitude of the merchant's location.\n",
    "* **`merch_long`:** Longitude of the merchant's location.\n",
    "* **`is_fraud`:** Indicates whether the transaction is fraudulent. This is the target feature I will predict.\n",
    "* **`merch_zipcode`:** Merchant's postal code.\n",
    "\n",
    "## 2.2. Explatory Data Analysis\n",
    "\n",
    "I am going to examine the first 5 rows of our dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:42:11.785282Z",
     "iopub.status.busy": "2024-10-27T19:42:11.784437Z",
     "iopub.status.idle": "2024-10-27T19:42:22.221484Z",
     "shell.execute_reply": "2024-10-27T19:42:22.220398Z",
     "shell.execute_reply.started": "2024-10-27T19:42:11.78524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('C:\\Users\\malik\\Desktop\\Hassan's Work\\Fraud detection\\credit_card_transactions.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.97365Z",
     "iopub.status.idle": "2024-10-27T14:56:58.974029Z",
     "shell.execute_reply": "2024-10-27T14:56:58.973867Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.973847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {data.shape[0]}\")\n",
    "print(f\"Number of columns: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.975144Z",
     "iopub.status.idle": "2024-10-27T14:56:58.975499Z",
     "shell.execute_reply": "2024-10-27T14:56:58.975336Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.975318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_types = pd.DataFrame(data.dtypes).reset_index()\n",
    "data_types.columns = ['Feature Name', 'Data Type'] \n",
    "data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining if there is any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.977457Z",
     "iopub.status.idle": "2024-10-27T14:56:58.977954Z",
     "shell.execute_reply": "2024-10-27T14:56:58.977729Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.977685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "missing_counts = data.isnull().sum()\n",
    "missing_counts_df = pd.DataFrame(missing_counts).reset_index()\n",
    "missing_counts_df.columns = ['Feature', 'Missing Value Count']\n",
    "missing_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of unique values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.97935Z",
     "iopub.status.idle": "2024-10-27T14:56:58.979834Z",
     "shell.execute_reply": "2024-10-27T14:56:58.979592Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.979569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unique_counts = data.nunique()\n",
    "unique_counts_df = pd.DataFrame(unique_counts).reset_index()\n",
    "unique_counts_df.columns = ['Feature', 'Number of Unique Values'] \n",
    "unique_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there is any duplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.981228Z",
     "iopub.status.idle": "2024-10-27T14:56:58.98172Z",
     "shell.execute_reply": "2024-10-27T14:56:58.981477Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.981452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f'Number of duplicated rows: {data.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Data Visualisation\n",
    "\n",
    "Visualization of the distribution of the `is_fraud` column indicating fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.983411Z",
     "iopub.status.idle": "2024-10-27T14:56:58.983897Z",
     "shell.execute_reply": "2024-10-27T14:56:58.983651Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.983628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='is_fraud', data=data, palette='viridis')\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='bottom', \n",
    "                fontsize=10, color='black')\n",
    "plt.title('is_fraud Distribution')\n",
    "plt.xlabel('is_fraud')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our dataset has a quite imbalanced distribution. Only the **7506** of the data **(% 0.58)** are fraduent.\n",
    "\n",
    "--\n",
    "\n",
    "Visualization of the `is_fraud` distribution based on some categorical variables using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.985954Z",
     "iopub.status.idle": "2024-10-27T14:56:58.986414Z",
     "shell.execute_reply": "2024-10-27T14:56:58.986195Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.98617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['category', 'gender']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.countplot(y=column, hue='is_fraud', data=data, palette='viridis')\n",
    "    plt.title(f'{column} Distribution by is_fraud')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(column)\n",
    "    plt.legend(title='credit approval', loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(True)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        ax.text(width + 10, p.get_y() + p.get_height() / 2, \n",
    "                f'{int(width)}', \n",
    "                ha='left', va='center')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histograms for some of the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.988793Z",
     "iopub.status.idle": "2024-10-27T14:56:58.989148Z",
     "shell.execute_reply": "2024-10-27T14:56:58.988994Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.988977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['amt', 'city_pop']\n",
    "\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(data[data['is_fraud'] == 1][column], bins=30, kde=True, color='red')\n",
    "    plt.title(f'Fraud {column} Distribution')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(data[data['is_fraud'] == 0][column], bins=30, kde=True, color='blue')\n",
    "    plt.title(f'Non-Fraud {column} Distribution')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing monthly and yearly fraud transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.990262Z",
     "iopub.status.idle": "2024-10-27T14:56:58.990592Z",
     "shell.execute_reply": "2024-10-27T14:56:58.990443Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.99042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_copy = data.copy()\n",
    "fraud_transactions = data_copy[data_copy['is_fraud'] == 1]\n",
    "non_fraud_transactions = data_copy[data_copy['is_fraud'] == 0]\n",
    "\n",
    "fraud_transactions['trans_date_trans_time'] = pd.to_datetime(fraud_transactions['trans_date_trans_time'])\n",
    "monthly_counts = fraud_transactions['trans_date_trans_time'].dt.to_period('M').value_counts().sort_index()\n",
    "fraud_transactions['season'] = fraud_transactions['trans_date_trans_time'].dt.month % 12 // 3 + 1\n",
    "fraud_transactions['year'] = fraud_transactions['trans_date_trans_time'].dt.year\n",
    "fraud_transactions['year_season'] = fraud_transactions['year'].astype(str) + ' - ' + fraud_transactions['season'].map({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Autumn'})\n",
    "seasonal_counts = fraud_transactions['year_season'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=monthly_counts.index.astype(str), y=monthly_counts.values)\n",
    "plt.title('Monthly Fraud Transactions')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, count in enumerate(monthly_counts.values):\n",
    "    plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=seasonal_counts.index, y=seasonal_counts.values)\n",
    "plt.title('Seasonal Fraud Transactions')\n",
    "plt.xlabel('Year - Season')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for i, count in enumerate(seasonal_counts.values):\n",
    "    plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there were no fraudulent transactions at all during the autumn of 2020.\n",
    "\n",
    "--\n",
    "\n",
    "Let's observe which hours have a higher occurrence of fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.992531Z",
     "iopub.status.idle": "2024-10-27T14:56:58.992925Z",
     "shell.execute_reply": "2024-10-27T14:56:58.992766Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.992747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fraud_transactions['hour'] = fraud_transactions['trans_date_trans_time'].dt.hour\n",
    "hourly_fraud_counts = fraud_transactions['hour'].value_counts().sort_index()\n",
    "average_hourly_fraud_counts = hourly_fraud_counts / fraud_transactions['hour'].value_counts().count()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=average_hourly_fraud_counts.index, y=average_hourly_fraud_counts.values)\n",
    "plt.title('Average Hourly Fraud Counts')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "for i, count in enumerate(average_hourly_fraud_counts.values):\n",
    "    plt.text(i, count, f\"{count:.1f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a large majority of fraudulent activities occurred **between 22:00 and 03:59**, particularly **between 22:00 and 23:59**.\n",
    "\n",
    "--\n",
    "\n",
    "Calculating and visualising the average number of fraud transactions for each day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.9943Z",
     "iopub.status.idle": "2024-10-27T14:56:58.994626Z",
     "shell.execute_reply": "2024-10-27T14:56:58.99448Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.994464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fraud_transactions['weekday'] = fraud_transactions['trans_date_trans_time'].dt.weekday\n",
    "weekday_fraud_counts = fraud_transactions['weekday'].value_counts().sort_index()\n",
    "average_weekday_fraud_counts = weekday_fraud_counts / fraud_transactions['weekday'].value_counts().count()\n",
    "days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=average_weekday_fraud_counts.index, y=average_weekday_fraud_counts.values)\n",
    "plt.title('Average Daily Fraud Transactions')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.xticks(ticks=average_weekday_fraud_counts.index, labels=days_of_week, rotation=0)\n",
    "\n",
    "# Her çubuğun üzerine ortalama değeri ekle\n",
    "for i, count in enumerate(average_weekday_fraud_counts.values):\n",
    "    plt.text(i, count, f\"{count:.1f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Manupilation\n",
    "\n",
    "## 3.1. Detecting and Removing Outliers\n",
    "\n",
    "The only numerical value I can use for outlier detection is the transaction amount (`amt`). Other numerical values, such as population, latitude, longitude, etc., do not hold any meaningful statistical significance or distribution in this context.\n",
    "\n",
    "I will consider data points above a certain `amt` value as outliers and remove them.\n",
    "\n",
    "To examine the structure of the `amt` column and decide on a suitable threshold, I need to plot a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:58.995782Z",
     "iopub.status.idle": "2024-10-27T14:56:58.996104Z",
     "shell.execute_reply": "2024-10-27T14:56:58.995956Z",
     "shell.execute_reply.started": "2024-10-27T14:56:58.99594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  \n",
    "plt.scatter(data.index, data['amt'], color='red') \n",
    "plt.title(\"Scatter Plot of 'amt'\")  \n",
    "plt.xlabel('Index')  \n",
    "plt.ylabel('amt')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon visual inspection, it is possible to see that the data points start to disperse after values around 2500 to 3000.\n",
    "\n",
    "Let's set the threshold at 2700 and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:42:35.447861Z",
     "iopub.status.busy": "2024-10-27T19:42:35.446782Z",
     "iopub.status.idle": "2024-10-27T19:42:51.424651Z",
     "shell.execute_reply": "2024-10-27T19:42:51.423629Z",
     "shell.execute_reply.started": "2024-10-27T19:42:35.447797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "outlier_threshold = 2700\n",
    "outliers = data['amt'] > outlier_threshold\n",
    "outlier_count = np.count_nonzero(outliers)\n",
    "total_count = len(data)\n",
    "outlier_percentage = (outlier_count / total_count) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data.index[outliers], data['amt'][outliers], color='lightcoral', label='Outliers')\n",
    "plt.scatter(data.index[~outliers], data['amt'][~outliers], color='red', label='Value Points')\n",
    "plt.axhline(y=outlier_threshold, color='black', linestyle='--', label=f'Outlier Threshold: {outlier_threshold}')\n",
    "plt.title('Outlier Analysis Using Scatter Plot')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('amt Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f'Number of outliers: {outlier_count}')\n",
    "print(f'Outlier percentage: % {outlier_percentage}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to remove all records where the amt value is greater than 2700. Only **430 entries** will be lost, resulting in a **0.03% data loss**. Given the dataset's size, this is not considered a significant loss of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:43:12.152534Z",
     "iopub.status.busy": "2024-10-27T19:43:12.151904Z",
     "iopub.status.idle": "2024-10-27T19:43:12.42827Z",
     "shell.execute_reply": "2024-10-27T19:43:12.426894Z",
     "shell.execute_reply.started": "2024-10-27T19:43:12.152494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = data[~outliers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Dropping Unnecessary Features\n",
    "\n",
    "Let's remove the following columns, as they are not necessary for training our model.\n",
    "\n",
    "* **`Unnamed: 0`** - It's a unique number similar to an index, which is meaningless for training purposes.\n",
    "* **`first`**  - The cardholder's first and last names are irrelevant for training purposes.\n",
    "* **`last`**  - The cardholder's first and last names are irrelevant for training purposes.\n",
    "* **`street`**  - Since we already have `lat` and `lon` information, we don't need the data in these columns.\n",
    "* **`city`**  - Since we already have `lat` and `lon` information, we don't need the data in these columns.\n",
    "* **`state`**  - Since we already have `lat` and `lon` information, we don't need the data in these columns.\n",
    "* **`zip`**  - Since we already have `lat` and `lon` information, we don't need the data in these columns.\n",
    "* **`trans_num`**  - It's a unique number for each transaction, which is meaningless for training purposes.\n",
    "* **`unix_time`**  - We can get the same information from 'trans_date_trans_time' column.\n",
    "* **`merch_zipcode`**  - Since we already have `lat` and `lon` information, we don't need the data in these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:43:16.242485Z",
     "iopub.status.busy": "2024-10-27T19:43:16.242098Z",
     "iopub.status.idle": "2024-10-27T19:43:16.419586Z",
     "shell.execute_reply": "2024-10-27T19:43:16.418779Z",
     "shell.execute_reply.started": "2024-10-27T19:43:16.242449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "to_drop = ['Unnamed: 0', 'first', 'last', 'street', 'city', 'state', 'zip', 'trans_num','unix_time','merch_zipcode']\n",
    "data = data.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Feature Creation\n",
    "\n",
    "We need to create the `trans_year`, `trans_month`, `trans_day`, `trans_season`, `trans_weekday`, `trans_hour`, `trans_minute`, and `trans_second` columns using the `trans_date_trans_time` column. Afterwards, we can remove the `trans_date_trans_time` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:43:19.740362Z",
     "iopub.status.busy": "2024-10-27T19:43:19.739509Z",
     "iopub.status.idle": "2024-10-27T19:43:20.600366Z",
     "shell.execute_reply": "2024-10-27T19:43:20.599545Z",
     "shell.execute_reply.started": "2024-10-27T19:43:19.740305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])\n",
    "data['trans_year'] = data['trans_date_trans_time'].dt.year\n",
    "data['trans_month'] = data['trans_date_trans_time'].dt.month\n",
    "data['trans_day'] = data['trans_date_trans_time'].dt.day\n",
    "data['trans_season'] = data['trans_date_trans_time'].dt.month % 12 // 3 + 1  #1 = Winter, 2 = Spring, 3 = Summer, 4 = Autumn\n",
    "data['trans_weekday'] = data['trans_date_trans_time'].dt.weekday \n",
    "data['trans_hour'] = data['trans_date_trans_time'].dt.hour\n",
    "data['trans_minute'] = data['trans_date_trans_time'].dt.minute\n",
    "data['trans_second'] = data['trans_date_trans_time'].dt.second\n",
    "\n",
    "data = data.drop(columns=['trans_date_trans_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the cardholder's age at the time the transaction occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:43:23.755589Z",
     "iopub.status.busy": "2024-10-27T19:43:23.754705Z",
     "iopub.status.idle": "2024-10-27T19:43:24.302652Z",
     "shell.execute_reply": "2024-10-27T19:43:24.301827Z",
     "shell.execute_reply.started": "2024-10-27T19:43:23.755537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['dob'] = pd.to_datetime(data['dob'])\n",
    "data['birth_year'] = data['dob'].dt.year\n",
    "data['card_holder_age'] = data['trans_year'] - data['birth_year']\n",
    "data = data.drop(columns=['dob', 'birth_year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will calculate the geographical distance between two points using their latitude and longitude coordinates using `geopy` library. The geopy library simplifies calculating geographical distances between points with high precision by providing easy-to-use methods that account for the Earth's curvature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:43:27.035281Z",
     "iopub.status.busy": "2024-10-27T19:43:27.034875Z",
     "iopub.status.idle": "2024-10-27T19:49:45.227122Z",
     "shell.execute_reply": "2024-10-27T19:49:45.226323Z",
     "shell.execute_reply.started": "2024-10-27T19:43:27.035238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_distance(row):\n",
    "    point_a = (row['lat'], row['long'])\n",
    "    point_b = (row['merch_lat'], row['merch_long'])\n",
    "    return geodesic(point_a, point_b).kilometers \n",
    "\n",
    "data['distance'] = data.apply(calculate_distance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Encoding\n",
    "\n",
    "Using `LabelEncoder` to convert categorical features into numerical values for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:49:56.002832Z",
     "iopub.status.busy": "2024-10-27T19:49:56.002202Z",
     "iopub.status.idle": "2024-10-27T19:49:57.432691Z",
     "shell.execute_reply": "2024-10-27T19:49:57.431563Z",
     "shell.execute_reply.started": "2024-10-27T19:49:56.002792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_categorical_columns(data, columns):\n",
    "    le = LabelEncoder()\n",
    "    for col in columns:\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "    return data\n",
    "\n",
    "cat_features = ['cc_num', 'merchant', 'category', 'gender', 'job']\n",
    "data = encode_categorical_columns(data, cat_features)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Splitting Data Into Train and Test Sets\n",
    "\n",
    "Let's split the data into training and test sets. To maintain class imbalance in both sets, I will use the `stratify` parameter during the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling\n",
    "\n",
    "## 4.1. Model Selection\n",
    "\n",
    "Here, I specifically defined the **CatBoost**, **XGBooost**, and **LightGBM** models. I tried to optimize the hyperparameters of the models beforehand using Optuna. Later, by tweaking the model's hyperparameters, it may be possible to improve the model's performance.\n",
    "\n",
    "I defined the `class_weights` object to ensure that each class is equally considered by the model in a dataset with imbalanced class distribution. The `compute_class_weight` function calculates weights based on class frequencies, and these weights are converted into a dictionary. This dictionary is used in the parameters of the `CatBoostClassifier`, `XGBClassifier`, and `LGBMClassifier` models to give more importance to underrepresented classes during the learning process. This approach helps mitigate the negative effects of class imbalance and enhances the overall performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T19:50:01.530503Z",
     "iopub.status.busy": "2024-10-27T19:50:01.530038Z",
     "iopub.status.idle": "2024-10-27T19:50:02.683525Z",
     "shell.execute_reply": "2024-10-27T19:50:02.682106Z",
     "shell.execute_reply.started": "2024-10-27T19:50:01.530464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = data.drop('is_fraud', axis=1)\n",
    "y = data['is_fraud']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1881)\n",
    "\n",
    "print(\"y_train class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "\n",
    "print(\"\\ny_test class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T20:59:35.058696Z",
     "iopub.status.busy": "2024-10-27T20:59:35.058233Z",
     "iopub.status.idle": "2024-10-27T20:59:35.403224Z",
     "shell.execute_reply": "2024-10-27T20:59:35.402137Z",
     "shell.execute_reply.started": "2024-10-27T20:59:35.058652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "models = {\n",
    "    \"Catboost\": CatBoostClassifier(\n",
    "        depth=10,\n",
    "        learning_rate=0.2,\n",
    "        n_estimators=2000,\n",
    "        min_child_samples=10,\n",
    "        subsample=0.7,\n",
    "        l2_leaf_reg=8,\n",
    "        cat_features=cat_features,\n",
    "        random_state=1881,\n",
    "        eval_metric='F1',\n",
    "        loss_function='Logloss',\n",
    "        bootstrap_type='Bernoulli',\n",
    "        class_weights=class_weights_dict,\n",
    "        task_type='GPU',\n",
    "        verbose=False\n",
    "    ),\n",
    "    # \"XGBoost\": XGBClassifier(\n",
    "    #     max_depth=7,\n",
    "    #     learning_rate=0.2,\n",
    "    #     n_estimators=2000,\n",
    "    #     min_child_weight=10,\n",
    "    #     subsample=0.8,\n",
    "    #     reg_lambda=1,\n",
    "    #     reg_alpha=3,\n",
    "    #     scale_pos_weight=class_weights_dict[1] / class_weights_dict[0],\n",
    "    #     objective='binary:logistic', \n",
    "    #     eval_metric='logloss',\n",
    "    #     tree_method='hist', \n",
    "    #     device='cuda',\n",
    "    #     random_state=1881,\n",
    "    #     verbose=False\n",
    "    # ),\n",
    "    # \"LGBM\": LGBMClassifier(\n",
    "    #     max_depth=8,\n",
    "    #     num_leaves=64,\n",
    "    #     learning_rate=0.03,\n",
    "    #     n_estimators=2000,\n",
    "    #     min_child_weight=10,\n",
    "    #     subsample=0.9,\n",
    "    #     reg_lambda=3,\n",
    "    #     reg_alpha=1,\n",
    "    #     scale_pos_weight=class_weights_dict[1] / class_weights_dict[0],\n",
    "    #     objective='binary',  \n",
    "    #     metric='binary_logloss',  \n",
    "    #     random_state=1881,\n",
    "    #     device=\"gpu\",\n",
    "    #     gpu_platform_id = 0,\n",
    "    #     gpu_device_id = 0,\n",
    "    #     verbose=-1\n",
    "    # )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Metric Selection\n",
    "\n",
    "As previously mentioned, our class variable has a highly imbalanced distribution, making the choice of evaluation metrics crucial for accurately assessing model performance. In this project, we will use the **F1 Score**, **Precision**, and **Recall** as our metrics to effectively capture the model's ability to identify the minority class while minimizing false positives and false negatives. \n",
    "\n",
    "The **F1 Score** provides a balance between precision and recall, offering a single measure that reflects both the accuracy of the positive predictions and the model's completeness in identifying all relevant instances. **Precision** measures the proportion of true positive predictions among all positive predictions, indicating how many of the predicted positive cases are actually correct. **Recall**, on the other hand, assesses the model's ability to identify all relevant instances, representing the proportion of true positives among all actual positive cases. By utilizing these metrics, we can better evaluate our model's performance in the context of imbalanced classes.\n",
    "\n",
    "Here, the training dataset is divided into 5 parts using **stratified k-fold**. Subsequently, the models are tested on each part, and the average **F1 Score**, **Recall**, and **Precision** values for each model are printed. This way, we gain insights into the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:59.012796Z",
     "iopub.status.idle": "2024-10-27T14:56:59.01314Z",
     "shell.execute_reply": "2024-10-27T14:56:59.012988Z",
     "shell.execute_reply.started": "2024-10-27T14:56:59.01297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {\n",
    "    model_name: {\n",
    "        \"y_tests\": [],  \n",
    "        \"y_preds\": [],  \n",
    "        \"f1_scores\": [],\n",
    "        \"precisions\": [],\n",
    "        \"recalls\": []\n",
    "    }\n",
    "    for model_name in models.keys()\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1881)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "\n",
    "        f1 = f1_score(y_test_fold, y_pred_fold)\n",
    "        precision = precision_score(y_test_fold, y_pred_fold)\n",
    "        recall = recall_score(y_test_fold, y_pred_fold)\n",
    "        \n",
    "        results[model_name][\"y_tests\"].append(y_test_fold)\n",
    "        results[model_name][\"y_preds\"].append(y_pred_fold)\n",
    "        results[model_name][\"f1_scores\"].append(f1)\n",
    "        results[model_name][\"precisions\"].append(precision)\n",
    "        results[model_name][\"recalls\"].append(recall)\n",
    "        \n",
    "average_results = {\n",
    "    \"Model\": [],\n",
    "    \"Mean F1 Score\": [],\n",
    "    \"Mean Precision\": [],\n",
    "    \"Mean Recall\": []\n",
    "}\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    average_results[\"Model\"].append(model_name)\n",
    "    average_results[\"Mean F1 Score\"].append(sum(metrics[\"f1_scores\"]) / len(metrics[\"f1_scores\"]))\n",
    "    average_results[\"Mean Precision\"].append(sum(metrics[\"precisions\"]) / len(metrics[\"precisions\"]))\n",
    "    average_results[\"Mean Recall\"].append(sum(metrics[\"recalls\"]) / len(metrics[\"recalls\"]))\n",
    "\n",
    "df_results = pd.DataFrame(average_results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Define the directory path in Google Drive\n",
    "save_directory = 'saved_models'\n",
    "\n",
    "# Create the directory if it doesn’t already exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save each model to the directory in Google Drive\n",
    "for model_name, model in models.items():\n",
    "    joblib.dump(model, os.path.join(save_directory, f'{model_name}_model.joblib'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Ensembling Catboost, XGBoost and LightGBM\n",
    "\n",
    "As observed, the performance of all three models is relatively good. Now, let's combine all the models using a **soft voting ensemble model** and train it on the training dataset. Afterwards, we will evaluate the performance of the new model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T20:47:41.548758Z",
     "iopub.status.busy": "2024-10-27T20:47:41.548288Z",
     "iopub.status.idle": "2024-10-27T20:57:53.862032Z",
     "shell.execute_reply": "2024-10-27T20:57:53.860743Z",
     "shell.execute_reply.started": "2024-10-27T20:47:41.548711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('Catboost', models[\"Catboost\"]),\n",
    "        ('XGBoost', models[\"XGBoost\"]),\n",
    "        ('LGBM', models[\"LGBM\"])\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "metrics = {\n",
    "    'Metric': ['F1 Score', 'Precision', 'Recall'],\n",
    "    'Value': [f1, precision, recall]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising the confusion matrix of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T20:06:55.951742Z",
     "iopub.status.busy": "2024-10-27T20:06:55.950987Z",
     "iopub.status.idle": "2024-10-27T20:06:56.327854Z",
     "shell.execute_reply": "2024-10-27T20:06:56.32675Z",
     "shell.execute_reply.started": "2024-10-27T20:06:55.951698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='plasma', cbar=True, linewidths=1, linecolor='black')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the classification report of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:21:49.085207Z",
     "iopub.status.busy": "2024-10-27T20:21:49.084712Z",
     "iopub.status.idle": "2024-10-27T20:21:49.961762Z",
     "shell.execute_reply": "2024-10-27T20:21:49.960338Z",
     "shell.execute_reply.started": "2024-10-27T20:21:49.08516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "rep_df = pd.DataFrame(clf_report).transpose()\n",
    "rep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Feature Importances\n",
    "\n",
    "Calculating and visualising the average feature importances from three machine learning models (CatBoost, XGBoost, and LGBM) to identify the top 10 most important features influencing the model predictions by creating a bar plot for better interpretation of which features are most impactful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-27T14:56:59.02Z",
     "iopub.status.idle": "2024-10-27T14:56:59.020343Z",
     "shell.execute_reply": "2024-10-27T14:56:59.020189Z",
     "shell.execute_reply.started": "2024-10-27T14:56:59.020171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "catboost_importance = models['Catboost'].feature_importances_\n",
    "xgboost_importance = models['XGBoost'].feature_importances_\n",
    "lgbm_importance = models['LGBM'].feature_importances_\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'CatBoost': catboost_importance,\n",
    "    'XGBoost': xgboost_importance,\n",
    "    'LGBM': lgbm_importance\n",
    "})\n",
    "importance_df['Average'] = importance_df[['CatBoost', 'XGBoost', 'LGBM']].mean(axis=1)\n",
    "top_features = importance_df.nlargest(10, 'Average')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Average', y='Feature', data=top_features, palette='viridis')\n",
    "plt.title('Top 10 Important Features')\n",
    "plt.xlabel('Average Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "This project aimed to detect credit card fraud using machine learning techniques. To achieve this I used three different machine learning models: CatBoost, XGBoost, and LightGBM. These models were then combined using a voting ensemble method to create the final model. The performance of the final model was promising, achieving an F1 Score of **%92**, Precision of **%93**, and Recall of **%91**. Additionally, I identified the top 10 features contributing to the model's predictions, highlighting their significance in fraud detection.\n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "1. **Further Model Optimization**: While the final model demonstrates good performance, additional optimization techniques such as more extensive hyperparameter tuning could further enhance its performance.\n",
    "\n",
    "2. **Feature Engineering**: Investigating additional features or transformations of existing features could provide new insights and improve model performance.\n",
    "\n",
    "3. **Real-World Testing**: Deploying the model in a real-world scenario and monitoring its performance over time will help assess its effectiveness and adaptability to new data patterns.\n",
    "\n",
    "4. **Model Interpretability**: Implementing model interpretation techniques, such as SHAP (SHapley Additive exPlanations), can help in understanding the model's decision-making process and building trust with stakeholders.\n",
    "\n",
    "5. **Exploration of Alternative Models**: Experimenting with other machine learning algorithms or deep learning approaches may yield better results or offer complementary insights.\n",
    "\n",
    "6. **Regular Updates**: Establishing a pipeline for regularly updating the model with new data will ensure its ongoing effectiveness in detecting fraudulent activities.\n",
    "\n",
    "By pursuing these next steps, the model can be further refined and its ability to combat credit card fraud effectively enhanced."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5430856,
     "sourceId": 9013505,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
